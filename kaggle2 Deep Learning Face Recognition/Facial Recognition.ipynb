{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithFileName(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithFileName, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        file_name = self.imgs[index][0].split('/')[-1]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_file_name = (original_tuple + (file_name,))\n",
    "        return tuple_with_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION DATA LOADER\n",
    "\n",
    "### Train Data Loader Medium\n",
    "#### This dataloader return (img, label, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, utils, datasets\n",
    "\n",
    "val_test_data_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                              transforms.CenterCrop(224),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                   std=[0.229, 0.224, 0.225])\n",
    "                                             ])\n",
    "\n",
    "\n",
    "validation_classification_dataset = datasets.ImageFolder(root='./data/validation_classification/medium', \n",
    "                                                                  transform=val_test_data_transform)\n",
    "\n",
    "validation_classification_dataloader = torch.utils.data.DataLoader(validation_classification_dataset,\n",
    "                                             batch_size=32,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_transform = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                           transforms.RandomHorizontalFlip(),\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                std=[0.229, 0.224, 0.225])\n",
    "                                          ])\n",
    "train_medium_dataset = datasets.ImageFolder(root='./data/train_data/medium', \n",
    "                                            transform=train_data_transform)\n",
    "\n",
    "train_medium_dataloader = torch.utils.data.DataLoader(train_medium_dataset,\n",
    "                                             batch_size=32, \n",
    "                                             shuffle=True,\n",
    "                                             num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize images in dataloader"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Architecture\n",
    "\n",
    "### Baseline model\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "# torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "# CONV: Output size = (W-K+2P)/S + 1\n",
    "# MAXPOOL: Output size = (Wâ€“K+2P)/S + 1.\n",
    "\n",
    "class BaseLineNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseLineNet, self).__init__()\n",
    "        self.batchsize = 32\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5,  stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU(inplace = True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=5,  stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(192)\n",
    "        self.relu2 = nn.ReLU(inplace = True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3,  stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(384)\n",
    "        self.relu3 = nn.ReLU(inplace = True)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3,  stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.relu4 = nn.ReLU(inplace = True)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,  stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.relu5 = nn.ReLU(inplace = True)\n",
    "        \n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(4096, 4096)\n",
    "        self.fcbn1 = nn.BatchNorm1d(4096)\n",
    "        self.relu6 = nn.ReLU(inplace = True)\n",
    "        \n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fcbn2 = nn.BatchNorm1d(4096)\n",
    "        self.relu7 = nn.ReLU(inplace = True)\n",
    "        \n",
    "        self.fc3 = nn.Linear(4096, 2300)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv > bn > relu > pool > bn\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # conv > bn > relu > pool\n",
    "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        # conv > bn > relu\n",
    "        x = self.relu3(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # conv > bn > relu\n",
    "        x = self.relu4(self.bn4(self.conv4(x)))\n",
    "        \n",
    "        # conv > bn > relu > pool\n",
    "        x = self.pool5(self.relu5(self.bn5((self.conv5(x)))))\n",
    "        \n",
    "        # flatten\n",
    "        x = x.view(-1, 4096)\n",
    "        \n",
    "        # fc > bn > relu\n",
    "        x = F.relu(self.relu6(self.fc1(x)))\n",
    "        \n",
    "        # fc > bn > relu\n",
    "        x = F.relu(self.relu7(self.fc2(x)))\n",
    "        # may not need this group\n",
    "        \n",
    "        # fc\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision   \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    " \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        # if there is down sample\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    " \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    " \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    " \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    " \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    " \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    " \n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    " \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    " \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    " \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    " \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    " \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    " \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    " \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    " \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    " \n",
    "    def __init__(self, block, layers, num_classes=2300, num_feat = 4096):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        \n",
    "        self.linear_label = nn.Linear(512 * block.expansion, num_classes, bias=False)\n",
    "        \n",
    "        # For creating the embedding to be passed into the Center Loss criterion\n",
    "        self.linear_closs = nn.Linear(512 * block.expansion, num_feat, bias=False)\n",
    "        self.relu_closs = nn.ReLU(inplace=True)\n",
    " \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    " \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    " \n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    " \n",
    "        return nn.Sequential(*layers)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    " \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    " \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        label_output = self.linear_label(x)\n",
    "        label_output = label_output/torch.norm(self.linear_label.weight, dim=1)\n",
    "        \n",
    "        \n",
    "        # Create the feature embedding for the Center Loss\n",
    "        closs_output = self.linear_closs(x)\n",
    "        closs_output = self.relu_closs(closs_output)\n",
    "\n",
    "        return closs_output, label_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet34(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet50(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Center Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        feat_dim (int): feature dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, feat_dim, device=torch.device('cpu')):\n",
    "        super(CenterLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = feat_dim\n",
    "        self.device = device\n",
    "        \n",
    "        self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).to(self.device))\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature matrix with shape (batch_size, feat_dim).\n",
    "            labels: ground truth labels with shape (batch_size).\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(1, -2, x, self.centers.t())\n",
    "\n",
    "        classes = torch.arange(self.num_classes).long().to(self.device)\n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "        dist = []\n",
    "        for i in range(batch_size):\n",
    "            value = distmat[i][mask[i]]\n",
    "            value = value.clamp(min=1e-12, max=1e+12) # for numerical stability\n",
    "            dist.append(value)\n",
    "        dist = torch.cat(dist)\n",
    "        loss = dist.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESNET - Center Loss - Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_closs(model, data_loader, test_loader, task='Classification'):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(numEpochs):\n",
    "        avg_loss = 0.0\n",
    "        for batch_num, data in enumerate(data_loader):\n",
    "            feats, labels = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            optimizer_label.zero_grad()\n",
    "            optimizer_closs.zero_grad()\n",
    "            \n",
    "            feature, outputs = model(feats)\n",
    "\n",
    "            l_loss = criterion_label(outputs, labels.long())\n",
    "#             c_loss = criterion_closs(feature, labels.long())\n",
    "#             loss = l_loss + closs_weight * c_loss\n",
    "            loss = l_loss\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer_label.step()\n",
    "#             # by doing so, weight_cent would not impact on the learning of centers\n",
    "#             for param in criterion_closs.parameters():\n",
    "#                 param.grad.data *= (1. / closs_weight)\n",
    "            optimizer_closs.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            if batch_num % 50 == 49:\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
    "                avg_loss = 0.0\n",
    "                PATH = './resnet.pth'\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                # test\n",
    "#                 if task == 'Classification':\n",
    "#                     val_loss, val_acc = test_classify_closs(model, test_loader)\n",
    "# #                     train_loss, train_acc = test_classify_closs(model, data_loader)\n",
    "# #                     print('Train Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
    "# #                           format(train_loss, train_acc, val_loss, val_acc))\n",
    "#                     print('Val Loss: {:.4f}\\tVal Accuracy: {:.4f}'.format(val_loss, val_acc))\n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "        \n",
    "        if task == 'Classification':\n",
    "            val_loss, val_acc = test_classify_closs(model, test_loader)\n",
    "            train_loss, train_acc = test_classify_closs(model, data_loader)\n",
    "            print('Train Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
    "                  format(train_loss, train_acc, val_loss, val_acc))\n",
    "        else:\n",
    "            test_verify(model, test_loader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classify_closs(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_num, data in enumerate(test_loader):\n",
    "        feats, labels = data[0].to(device), data[1].to(device)\n",
    "        feature, outputs = model(feats)\n",
    "        \n",
    "        _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        \n",
    "        l_loss = criterion_label(outputs, labels.long())\n",
    "        #c_loss = criterion_closs(feature, labels.long())\n",
    "        #loss = l_loss + closs_weight * c_loss\n",
    "        loss=l_loss\n",
    "        accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        total += len(labels)\n",
    "        test_loss.extend([loss.item()]*feats.size()[0])\n",
    "        del feats\n",
    "        del labels\n",
    "\n",
    "    model.train()\n",
    "    return np.mean(test_loss), accuracy/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes =  2300\n"
     ]
    }
   ],
   "source": [
    "numEpochs = 10\n",
    "num_feats = 3\n",
    "\n",
    "learningRate = 1e-2\n",
    "weightDecay = 5e-5\n",
    "\n",
    "num_classes = len(train_medium_dataset.classes)\n",
    "print(\"num_classes = \", num_classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "closs_weight = 1\n",
    "lr_cent = 0.5\n",
    "feat_dim = 10\n",
    "\n",
    "network = resnet18()\n",
    "# network.load_state_dict(torch.load('./resnet_2hidden.pth'))\n",
    "# network.apply(init_weights)\n",
    "\n",
    "criterion_label = nn.CrossEntropyLoss()\n",
    "#criterion_closs = CenterLoss(num_classes, feat_dim, device)\n",
    "optimizer_label = torch.optim.SGD(network.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
    "#optimizer_closs = torch.optim.SGD(criterion_closs.parameters(), lr=lr_cent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 50\tAvg-Loss: 8.3198\n",
      "Epoch: 1\tBatch: 100\tAvg-Loss: 8.1797\n",
      "Epoch: 1\tBatch: 150\tAvg-Loss: 7.9238\n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 7.8142\n",
      "Epoch: 1\tBatch: 250\tAvg-Loss: 7.7764\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 7.7676\n",
      "Epoch: 1\tBatch: 350\tAvg-Loss: 7.7535\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 7.7416\n",
      "Epoch: 1\tBatch: 450\tAvg-Loss: 7.7336\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 7.7435\n",
      "Epoch: 1\tBatch: 550\tAvg-Loss: 7.7220\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 7.7221\n",
      "Epoch: 1\tBatch: 650\tAvg-Loss: 7.7122\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 7.6960\n",
      "Epoch: 1\tBatch: 750\tAvg-Loss: 7.7019\n",
      "Epoch: 1\tBatch: 800\tAvg-Loss: 7.6990\n",
      "Epoch: 1\tBatch: 850\tAvg-Loss: 7.6941\n",
      "Epoch: 1\tBatch: 900\tAvg-Loss: 7.6853\n",
      "Epoch: 1\tBatch: 950\tAvg-Loss: 7.6859\n",
      "Epoch: 1\tBatch: 1000\tAvg-Loss: 7.6746\n",
      "Epoch: 1\tBatch: 1050\tAvg-Loss: 7.6886\n",
      "Epoch: 1\tBatch: 1100\tAvg-Loss: 7.6747\n",
      "Epoch: 1\tBatch: 1150\tAvg-Loss: 7.6449\n",
      "Epoch: 1\tBatch: 1200\tAvg-Loss: 7.6616\n",
      "Epoch: 1\tBatch: 1250\tAvg-Loss: 7.6738\n",
      "Epoch: 1\tBatch: 1300\tAvg-Loss: 7.6582\n",
      "Epoch: 1\tBatch: 1350\tAvg-Loss: 7.6526\n",
      "Epoch: 1\tBatch: 1400\tAvg-Loss: 7.6484\n",
      "Epoch: 1\tBatch: 1450\tAvg-Loss: 7.6658\n",
      "Epoch: 1\tBatch: 1500\tAvg-Loss: 7.6187\n",
      "Epoch: 1\tBatch: 1550\tAvg-Loss: 7.6256\n",
      "Epoch: 1\tBatch: 1600\tAvg-Loss: 7.6432\n",
      "Epoch: 1\tBatch: 1650\tAvg-Loss: 7.6358\n",
      "Epoch: 1\tBatch: 1700\tAvg-Loss: 7.5979\n",
      "Epoch: 1\tBatch: 1750\tAvg-Loss: 7.6366\n",
      "Epoch: 1\tBatch: 1800\tAvg-Loss: 7.6069\n",
      "Epoch: 1\tBatch: 1850\tAvg-Loss: 7.6098\n",
      "Epoch: 1\tBatch: 1900\tAvg-Loss: 7.5737\n",
      "Epoch: 1\tBatch: 1950\tAvg-Loss: 7.5802\n",
      "Epoch: 1\tBatch: 2000\tAvg-Loss: 7.5976\n",
      "Epoch: 1\tBatch: 2050\tAvg-Loss: 7.5995\n",
      "Epoch: 1\tBatch: 2100\tAvg-Loss: 7.6127\n",
      "Epoch: 1\tBatch: 2150\tAvg-Loss: 7.5730\n",
      "Epoch: 1\tBatch: 2200\tAvg-Loss: 7.6072\n",
      "Epoch: 1\tBatch: 2250\tAvg-Loss: 7.5541\n",
      "Epoch: 1\tBatch: 2300\tAvg-Loss: 7.5692\n",
      "Epoch: 1\tBatch: 2350\tAvg-Loss: 7.5521\n",
      "Epoch: 1\tBatch: 2400\tAvg-Loss: 7.5457\n",
      "Epoch: 1\tBatch: 2450\tAvg-Loss: 7.5438\n",
      "Epoch: 1\tBatch: 2500\tAvg-Loss: 7.5279\n",
      "Epoch: 1\tBatch: 2550\tAvg-Loss: 7.5206\n",
      "Epoch: 1\tBatch: 2600\tAvg-Loss: 7.5329\n",
      "Epoch: 1\tBatch: 2650\tAvg-Loss: 7.5491\n",
      "Epoch: 1\tBatch: 2700\tAvg-Loss: 7.5278\n",
      "Epoch: 1\tBatch: 2750\tAvg-Loss: 7.5199\n",
      "Epoch: 1\tBatch: 2800\tAvg-Loss: 7.5233\n",
      "Epoch: 1\tBatch: 2850\tAvg-Loss: 7.5233\n",
      "Epoch: 1\tBatch: 2900\tAvg-Loss: 7.5382\n",
      "Epoch: 1\tBatch: 2950\tAvg-Loss: 7.5163\n",
      "Epoch: 1\tBatch: 3000\tAvg-Loss: 7.4975\n",
      "Epoch: 1\tBatch: 3050\tAvg-Loss: 7.5213\n",
      "Epoch: 1\tBatch: 3100\tAvg-Loss: 7.5231\n",
      "Epoch: 1\tBatch: 3150\tAvg-Loss: 7.5071\n",
      "Epoch: 1\tBatch: 3200\tAvg-Loss: 7.4757\n",
      "Epoch: 1\tBatch: 3250\tAvg-Loss: 7.5079\n",
      "Epoch: 1\tBatch: 3300\tAvg-Loss: 7.4964\n",
      "Epoch: 1\tBatch: 3350\tAvg-Loss: 7.4913\n",
      "Epoch: 1\tBatch: 3400\tAvg-Loss: 7.4702\n",
      "Epoch: 1\tBatch: 3450\tAvg-Loss: 7.4694\n",
      "Epoch: 1\tBatch: 3500\tAvg-Loss: 7.4621\n",
      "Epoch: 1\tBatch: 3550\tAvg-Loss: 7.4983\n",
      "Epoch: 1\tBatch: 3600\tAvg-Loss: 7.5045\n",
      "Epoch: 1\tBatch: 3650\tAvg-Loss: 7.4863\n",
      "Epoch: 1\tBatch: 3700\tAvg-Loss: 7.4787\n",
      "Epoch: 1\tBatch: 3750\tAvg-Loss: 7.4520\n",
      "Epoch: 1\tBatch: 3800\tAvg-Loss: 7.4669\n",
      "Epoch: 1\tBatch: 3850\tAvg-Loss: 7.4399\n",
      "Epoch: 1\tBatch: 3900\tAvg-Loss: 7.4519\n",
      "Epoch: 1\tBatch: 3950\tAvg-Loss: 7.4631\n",
      "Epoch: 1\tBatch: 4000\tAvg-Loss: 7.4511\n",
      "Epoch: 1\tBatch: 4050\tAvg-Loss: 7.4580\n",
      "Epoch: 1\tBatch: 4100\tAvg-Loss: 7.4170\n",
      "Epoch: 1\tBatch: 4150\tAvg-Loss: 7.4409\n",
      "Epoch: 1\tBatch: 4200\tAvg-Loss: 7.4131\n",
      "Epoch: 1\tBatch: 4250\tAvg-Loss: 7.4214\n",
      "Epoch: 1\tBatch: 4300\tAvg-Loss: 7.4458\n",
      "Epoch: 1\tBatch: 4350\tAvg-Loss: 7.4259\n",
      "Epoch: 1\tBatch: 4400\tAvg-Loss: 7.4127\n",
      "Epoch: 1\tBatch: 4450\tAvg-Loss: 7.3833\n",
      "Epoch: 1\tBatch: 4500\tAvg-Loss: 7.4063\n",
      "Epoch: 1\tBatch: 4550\tAvg-Loss: 7.3735\n",
      "Epoch: 1\tBatch: 4600\tAvg-Loss: 7.3823\n",
      "Epoch: 1\tBatch: 4650\tAvg-Loss: 7.4267\n",
      "Epoch: 1\tBatch: 4700\tAvg-Loss: 7.3921\n",
      "Epoch: 1\tBatch: 4750\tAvg-Loss: 7.3891\n",
      "Epoch: 1\tBatch: 4800\tAvg-Loss: 7.3692\n",
      "Epoch: 1\tBatch: 4850\tAvg-Loss: 7.3523\n",
      "Epoch: 1\tBatch: 4900\tAvg-Loss: 7.3443\n",
      "Epoch: 1\tBatch: 4950\tAvg-Loss: 7.3603\n",
      "Epoch: 1\tBatch: 5000\tAvg-Loss: 7.3793\n",
      "Epoch: 1\tBatch: 5050\tAvg-Loss: 7.3346\n",
      "Epoch: 1\tBatch: 5100\tAvg-Loss: 7.3610\n",
      "Epoch: 1\tBatch: 5150\tAvg-Loss: 7.3673\n",
      "Epoch: 1\tBatch: 5200\tAvg-Loss: 7.2973\n",
      "Epoch: 1\tBatch: 5250\tAvg-Loss: 7.3146\n",
      "Epoch: 1\tBatch: 5300\tAvg-Loss: 7.3430\n",
      "Epoch: 1\tBatch: 5350\tAvg-Loss: 7.3430\n",
      "Epoch: 1\tBatch: 5400\tAvg-Loss: 7.3158\n",
      "Epoch: 1\tBatch: 5450\tAvg-Loss: 7.2594\n",
      "Epoch: 1\tBatch: 5500\tAvg-Loss: 7.2992\n",
      "Epoch: 1\tBatch: 5550\tAvg-Loss: 7.3133\n",
      "Epoch: 1\tBatch: 5600\tAvg-Loss: 7.3070\n",
      "Epoch: 1\tBatch: 5650\tAvg-Loss: 7.3038\n",
      "Epoch: 1\tBatch: 5700\tAvg-Loss: 7.2510\n",
      "Epoch: 1\tBatch: 5750\tAvg-Loss: 7.2760\n",
      "Epoch: 1\tBatch: 5800\tAvg-Loss: 7.2837\n",
      "Epoch: 1\tBatch: 5850\tAvg-Loss: 7.2988\n",
      "Epoch: 1\tBatch: 5900\tAvg-Loss: 7.2463\n",
      "Epoch: 1\tBatch: 5950\tAvg-Loss: 7.2588\n",
      "Epoch: 1\tBatch: 6000\tAvg-Loss: 7.2297\n",
      "Epoch: 1\tBatch: 6050\tAvg-Loss: 7.2640\n",
      "Epoch: 1\tBatch: 6100\tAvg-Loss: 7.2149\n",
      "Epoch: 1\tBatch: 6150\tAvg-Loss: 7.2402\n",
      "Epoch: 1\tBatch: 6200\tAvg-Loss: 7.1850\n",
      "Epoch: 1\tBatch: 6250\tAvg-Loss: 7.2586\n",
      "Epoch: 1\tBatch: 6300\tAvg-Loss: 7.1948\n",
      "Epoch: 1\tBatch: 6350\tAvg-Loss: 7.2321\n",
      "Epoch: 1\tBatch: 6400\tAvg-Loss: 7.2366\n",
      "Epoch: 1\tBatch: 6450\tAvg-Loss: 7.2097\n",
      "Epoch: 1\tBatch: 6500\tAvg-Loss: 7.2279\n",
      "Epoch: 1\tBatch: 6550\tAvg-Loss: 7.2615\n",
      "Epoch: 1\tBatch: 6600\tAvg-Loss: 7.1818\n",
      "Epoch: 1\tBatch: 6650\tAvg-Loss: 7.2429\n",
      "Epoch: 1\tBatch: 6700\tAvg-Loss: 7.1100\n",
      "Epoch: 1\tBatch: 6750\tAvg-Loss: 7.1622\n",
      "Epoch: 1\tBatch: 6800\tAvg-Loss: 7.1814\n",
      "Epoch: 1\tBatch: 6850\tAvg-Loss: 7.1466\n",
      "Epoch: 1\tBatch: 6900\tAvg-Loss: 7.1403\n",
      "Epoch: 1\tBatch: 6950\tAvg-Loss: 7.1818\n",
      "Epoch: 1\tBatch: 7000\tAvg-Loss: 7.1386\n",
      "Epoch: 1\tBatch: 7050\tAvg-Loss: 7.1687\n",
      "Epoch: 1\tBatch: 7100\tAvg-Loss: 7.1473\n",
      "Epoch: 1\tBatch: 7150\tAvg-Loss: 7.0936\n",
      "Epoch: 1\tBatch: 7200\tAvg-Loss: 7.1310\n",
      "Epoch: 1\tBatch: 7250\tAvg-Loss: 7.0889\n",
      "Epoch: 1\tBatch: 7300\tAvg-Loss: 7.1239\n",
      "Epoch: 1\tBatch: 7350\tAvg-Loss: 7.1400\n",
      "Epoch: 1\tBatch: 7400\tAvg-Loss: 7.0912\n",
      "Epoch: 1\tBatch: 7450\tAvg-Loss: 7.1029\n",
      "Epoch: 1\tBatch: 7500\tAvg-Loss: 7.1003\n",
      "Epoch: 1\tBatch: 7550\tAvg-Loss: 7.0808\n",
      "Epoch: 1\tBatch: 7600\tAvg-Loss: 7.0709\n",
      "Epoch: 1\tBatch: 7650\tAvg-Loss: 7.1078\n",
      "Epoch: 1\tBatch: 7700\tAvg-Loss: 7.1211\n",
      "Epoch: 1\tBatch: 7750\tAvg-Loss: 7.0199\n",
      "Epoch: 1\tBatch: 7800\tAvg-Loss: 7.0446\n",
      "Epoch: 1\tBatch: 7850\tAvg-Loss: 7.0396\n",
      "Epoch: 1\tBatch: 7900\tAvg-Loss: 7.0782\n",
      "Epoch: 1\tBatch: 7950\tAvg-Loss: 7.0662\n",
      "Epoch: 1\tBatch: 8000\tAvg-Loss: 7.0535\n",
      "Epoch: 1\tBatch: 8050\tAvg-Loss: 7.0104\n",
      "Epoch: 1\tBatch: 8100\tAvg-Loss: 7.0054\n",
      "Epoch: 1\tBatch: 8150\tAvg-Loss: 7.0214\n",
      "Epoch: 1\tBatch: 8200\tAvg-Loss: 7.0519\n",
      "Epoch: 1\tBatch: 8250\tAvg-Loss: 7.0386\n",
      "Epoch: 1\tBatch: 8300\tAvg-Loss: 7.0188\n",
      "Epoch: 1\tBatch: 8350\tAvg-Loss: 6.9976\n",
      "Epoch: 1\tBatch: 8400\tAvg-Loss: 7.0007\n",
      "Epoch: 1\tBatch: 8450\tAvg-Loss: 6.9405\n",
      "Epoch: 1\tBatch: 8500\tAvg-Loss: 6.9421\n",
      "Epoch: 1\tBatch: 8550\tAvg-Loss: 7.0107\n",
      "Epoch: 1\tBatch: 8600\tAvg-Loss: 6.9801\n",
      "Epoch: 1\tBatch: 8650\tAvg-Loss: 6.9430\n",
      "Epoch: 1\tBatch: 8700\tAvg-Loss: 6.9195\n",
      "Epoch: 1\tBatch: 8750\tAvg-Loss: 6.9719\n",
      "Epoch: 1\tBatch: 8800\tAvg-Loss: 6.9878\n",
      "Epoch: 1\tBatch: 8850\tAvg-Loss: 6.9633\n",
      "Epoch: 1\tBatch: 8900\tAvg-Loss: 6.9404\n",
      "Epoch: 1\tBatch: 8950\tAvg-Loss: 6.9505\n",
      "Epoch: 1\tBatch: 9000\tAvg-Loss: 6.9254\n",
      "Epoch: 1\tBatch: 9050\tAvg-Loss: 6.9406\n",
      "Epoch: 1\tBatch: 9100\tAvg-Loss: 6.9100\n",
      "Epoch: 1\tBatch: 9150\tAvg-Loss: 6.9097\n",
      "Epoch: 1\tBatch: 9200\tAvg-Loss: 6.9250\n",
      "Epoch: 1\tBatch: 9250\tAvg-Loss: 6.8600\n",
      "Epoch: 1\tBatch: 9300\tAvg-Loss: 6.9194\n",
      "Epoch: 1\tBatch: 9350\tAvg-Loss: 6.8568\n",
      "Epoch: 1\tBatch: 9400\tAvg-Loss: 6.8885\n",
      "Epoch: 1\tBatch: 9450\tAvg-Loss: 6.8689\n",
      "Epoch: 1\tBatch: 9500\tAvg-Loss: 6.8852\n",
      "Epoch: 1\tBatch: 9550\tAvg-Loss: 6.8875\n",
      "Epoch: 1\tBatch: 9600\tAvg-Loss: 6.8217\n",
      "Epoch: 1\tBatch: 9650\tAvg-Loss: 6.8759\n",
      "Epoch: 1\tBatch: 9700\tAvg-Loss: 6.8395\n",
      "Epoch: 1\tBatch: 9750\tAvg-Loss: 6.9061\n",
      "Epoch: 1\tBatch: 9800\tAvg-Loss: 6.8168\n",
      "Epoch: 1\tBatch: 9850\tAvg-Loss: 6.8967\n",
      "Epoch: 1\tBatch: 9900\tAvg-Loss: 6.7812\n",
      "Epoch: 1\tBatch: 9950\tAvg-Loss: 6.7764\n",
      "Epoch: 1\tBatch: 10000\tAvg-Loss: 6.7856\n",
      "Epoch: 1\tBatch: 10050\tAvg-Loss: 6.8850\n",
      "Epoch: 1\tBatch: 10100\tAvg-Loss: 6.8193\n",
      "Epoch: 1\tBatch: 10150\tAvg-Loss: 6.8079\n",
      "Epoch: 1\tBatch: 10200\tAvg-Loss: 6.8411\n",
      "Epoch: 1\tBatch: 10250\tAvg-Loss: 6.8615\n",
      "Epoch: 1\tBatch: 10300\tAvg-Loss: 6.7993\n",
      "Epoch: 1\tBatch: 10350\tAvg-Loss: 6.7761\n",
      "Epoch: 1\tBatch: 10400\tAvg-Loss: 6.7488\n",
      "Epoch: 1\tBatch: 10450\tAvg-Loss: 6.7131\n",
      "Epoch: 1\tBatch: 10500\tAvg-Loss: 6.7914\n",
      "Epoch: 1\tBatch: 10550\tAvg-Loss: 6.7328\n",
      "Epoch: 1\tBatch: 10600\tAvg-Loss: 6.7436\n",
      "Epoch: 1\tBatch: 10650\tAvg-Loss: 6.7064\n",
      "Epoch: 1\tBatch: 10700\tAvg-Loss: 6.7475\n",
      "Epoch: 1\tBatch: 10750\tAvg-Loss: 6.6948\n",
      "Epoch: 1\tBatch: 10800\tAvg-Loss: 6.7023\n",
      "Epoch: 1\tBatch: 10850\tAvg-Loss: 6.7298\n",
      "Epoch: 1\tBatch: 10900\tAvg-Loss: 6.6693\n",
      "Epoch: 1\tBatch: 10950\tAvg-Loss: 6.6918\n",
      "Epoch: 1\tBatch: 11000\tAvg-Loss: 6.7115\n",
      "Epoch: 1\tBatch: 11050\tAvg-Loss: 6.6664\n",
      "Epoch: 1\tBatch: 11100\tAvg-Loss: 6.6656\n",
      "Epoch: 1\tBatch: 11150\tAvg-Loss: 6.6820\n",
      "Epoch: 1\tBatch: 11200\tAvg-Loss: 6.7247\n",
      "Epoch: 1\tBatch: 11250\tAvg-Loss: 6.7019\n",
      "Epoch: 1\tBatch: 11300\tAvg-Loss: 6.6753\n",
      "Epoch: 1\tBatch: 11350\tAvg-Loss: 6.6315\n",
      "Epoch: 1\tBatch: 11400\tAvg-Loss: 6.6145\n",
      "Epoch: 1\tBatch: 11450\tAvg-Loss: 6.6595\n",
      "Epoch: 1\tBatch: 11500\tAvg-Loss: 6.6681\n",
      "Epoch: 1\tBatch: 11550\tAvg-Loss: 6.6099\n",
      "Epoch: 1\tBatch: 11600\tAvg-Loss: 6.5994\n",
      "Epoch: 1\tBatch: 11650\tAvg-Loss: 6.5742\n",
      "Epoch: 1\tBatch: 11700\tAvg-Loss: 6.6138\n",
      "Epoch: 1\tBatch: 11750\tAvg-Loss: 6.6329\n",
      "Epoch: 1\tBatch: 11800\tAvg-Loss: 6.6085\n",
      "Epoch: 1\tBatch: 11850\tAvg-Loss: 6.5563\n",
      "Epoch: 1\tBatch: 11900\tAvg-Loss: 6.6030\n",
      "Epoch: 1\tBatch: 11950\tAvg-Loss: 6.6100\n",
      "Epoch: 1\tBatch: 12000\tAvg-Loss: 6.5642\n",
      "Epoch: 1\tBatch: 12050\tAvg-Loss: 6.5770\n",
      "Epoch: 1\tBatch: 12100\tAvg-Loss: 6.5970\n",
      "Epoch: 1\tBatch: 12150\tAvg-Loss: 6.6304\n",
      "Epoch: 1\tBatch: 12200\tAvg-Loss: 6.5105\n",
      "Epoch: 1\tBatch: 12250\tAvg-Loss: 6.5635\n",
      "Epoch: 1\tBatch: 12300\tAvg-Loss: 6.5451\n",
      "Epoch: 1\tBatch: 12350\tAvg-Loss: 6.5960\n",
      "Epoch: 1\tBatch: 12400\tAvg-Loss: 6.5209\n",
      "Epoch: 1\tBatch: 12450\tAvg-Loss: 6.6295\n",
      "Epoch: 1\tBatch: 12500\tAvg-Loss: 6.5981\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-94056f916d2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_closs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_medium_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_classification_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-120-b35e64592643>\u001b[0m in \u001b[0;36mtrain_closs\u001b[0;34m(model, data_loader, test_loader, task)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0moptimizer_closs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m49\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "network.train()\n",
    "network.to(device)\n",
    "train_closs(network, train_medium_dataloader, validation_classification_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 6.4249\tVal Accuracy: 0.0233\n"
     ]
    }
   ],
   "source": [
    "network = resnet18()\n",
    "network.load_state_dict(torch.load('./resnet.pth'))\n",
    "network.to(device)\n",
    "val_loss, val_acc = test_classify_closs(network, validation_classification_dataloader)\n",
    "print('Val Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
    "      format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSS Function and Optimizer\n",
    "1. Trained using SGD with lr=0.01 with Nesterov momentum = 0.9 and weight decay = 5e-4 with a batch size = 256\n",
    "2. StepLR scheduler with learning rate decayed by a multiplicative factor of gamma = 0.9 after every epoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "# 3*5\n",
    "# sample number * embeded face\n",
    "print(input)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "# 3\n",
    "# sample number (label in it)\n",
    "print(target)\n",
    "output = loss(input, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1568) to match target batch_size (32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-fe455ead40fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2007\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1834\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 1836\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   1837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1568) to match target batch_size (32)."
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "epoch_num = 10\n",
    "print_interval = 100\n",
    "net = BaseLineNet()\n",
    "net.to(device)\n",
    "net.train()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "# net.load_state_dict(torch.load('./baseline_bn.pth'))\n",
    "\n",
    "\n",
    "for epoch in range(epoch_num):# loop over the dataset multiple times\n",
    "    scheduler.step()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    for i, data in enumerate(train_medium_dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % print_interval == 1:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / i))\n",
    "            print('Accuracy of the network : %.8f %%' % (100 * correct / total))\n",
    "            \n",
    "            PATH = './baseline_bn.pth'\n",
    "            torch.save(net.state_dict(), PATH)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 140, in __getitem__\n    sample = self.transform(sample)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 70, in __call__\n    img = t(img)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 207, in __call__\n    return F.resize(img, self.size, self.interpolation)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 238, in resize\n    raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\nTypeError: img should be PIL Image. Got <class 'torch.Tensor'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-229bb2823898>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation_classification_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 140, in __getitem__\n    sample = self.transform(sample)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 70, in __call__\n    img = t(img)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 207, in __call__\n    return F.resize(img, self.size, self.interpolation)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 238, in resize\n    raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\nTypeError: img should be PIL Image. Got <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "net = BaseLineNet()\n",
    "net.load_state_dict(torch.load('./baseline_bn1.pth'))\n",
    "net.to(device)\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in validation_classification_dataloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        print()\n",
    "print(correct, total)\n",
    "print(\"Accuracy of the network on the test images: {0}\".format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and IO for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_classification_dataset = datasets.ImageFolder(root='./data/test_classification', \n",
    "#                                                    transform=val_test_data_transform)\n",
    "test_classification_dataset = ImageFolderWithFileName(root='./data/test_classification', \n",
    "                                                   transform=val_test_data_transform)\n",
    "test_classification_dataloader = torch.utils.data.DataLoader(test_classification_dataset,\n",
    "                                             batch_size=256, \n",
    "                                             shuffle=False,\n",
    "                                             num_workers=4)\n",
    "\n",
    "net = BaseLineNet()\n",
    "net.to(device)\n",
    "\n",
    "net.load_state_dict(torch.load('./baseline_bn1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pandas.core.frame import DataFrame\n",
    "\n",
    "a = range(2300)\n",
    "b = [str(i) for i in a]\n",
    "b.sort()\n",
    "idx_to_class = dict(zip(a,b))\n",
    "\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_classification_dataloader):\n",
    "        images, labels, file_name = data[0].to(device), data[1].to(device), data[2]\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        predicted_class = []\n",
    "        for j, idx in enumerate(predicted):\n",
    "            predicted_class.append(idx_to_class[idx.item()])\n",
    "\n",
    "        dict_data = {'Id': list(file_name), 'Category': predicted_class}\n",
    "        df_ = DataFrame(dict_data)\n",
    "        if i == 0:\n",
    "            df = df_\n",
    "        else:\n",
    "            df = pd.concat([df, df_])\n",
    "        \n",
    "df.to_csv('out_classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IO for verification\n",
    "#### Store the path of two image and the label in three lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def verification_pipeline(file_path, file_root, validation=True):\n",
    "    if validation:\n",
    "        df = pd.read_csv(file_path, sep=' ', header=None, names=['image1', 'image2', 'result'])\n",
    "    else:\n",
    "        df = pd.read_csv(file_path, sep=' ', header=None, names=['image1', 'image2'])\n",
    "    df['image1'] = file_root + df['image1']\n",
    "    df['image2'] = file_root + df['image2']\n",
    "    image_1 = df['image1'].tolist()\n",
    "    image_2 = df['image2'].tolist()\n",
    "    if validation:\n",
    "        is_same = df['result'].tolist()\n",
    "    else:\n",
    "        is_same = [-1] * df.shape[0]\n",
    "    return image_1, image_2, is_same\n",
    "\n",
    "\n",
    "vv_file_path = './data/validation_trials_verification.txt'\n",
    "tv_file_path = './data/test_trials_verification_student.txt'\n",
    "vv_root_path = \"./data/validation_verification/\"\n",
    "tv_root_path = \"./data/test_verification/\"\n",
    "\n",
    "vv_image_one, vv_image_two, vv_issame = verification_pipeline(vv_file_path, vv_root_path, True)\n",
    "tv_image_one, tv_image_two, tv_issame = verification_pipeline(tv_file_path, tv_root_path, validation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vv_image_one[100])\n",
    "print(vv_image_two[100])\n",
    "print(vv_issame[100])\n",
    "\n",
    "print(tv_image_one[100])\n",
    "print(tv_image_two[100])\n",
    "print(tv_issame[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized dataloader return two imgs and a bool lable for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "class Verification_dataset(Dataset):#need to inherit data.Dataset\n",
    "    \n",
    "    def __init__(self, image_one, image_two, is_same_person, transform=None):\n",
    "\n",
    "        self.image_one_list = image_one\n",
    "        self.image_two_list = image_two\n",
    "        self.is_same_person_list = is_same_person\n",
    "\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "        self.transform = transform\n",
    "\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        im1 = Image.open(self.image_one_list[i])\n",
    "        if self.transform is not None:\n",
    "            im1 = self.transform(im1)\n",
    "        \n",
    "        im2 = Image.open(self.image_two_list[i])\n",
    "        if self.transform is not None:\n",
    "            im2 = self.transform(im2)\n",
    "        \n",
    "        issame = self.is_same_person_list[i]\n",
    "\n",
    "        return im1, im2, issame\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.image_one_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "class CompareDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading compare face images\"\"\"\n",
    "\n",
    "    def __init__(self, txt_path, img_dir, transform=None):\n",
    "    \n",
    "        df = pd.read_csv(txt_path, sep=\" \", header=None, names=['img1', 'img2'])\n",
    "        self.img_dir = img_dir\n",
    "        self.txt_path = txt_path\n",
    "        self.img1_names = df['img1'].index.values\n",
    "        self.img2_names = df['img2'].index.values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img1 = Image.open(os.path.join(self.img_dir,\n",
    "                                       self.img1_names[index]))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1)\n",
    "        \n",
    "        img2 = Image.open(os.path.join(self.img_dir,\n",
    "                                       self.img2_names[index]))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img2 = self.transform(img2)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.img1_names.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_dataset = CompareDataset('./data/test_trials_verification_student.txt', './data/test_verification/', val_test_data_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification Validation Dataloader\n",
    "### load the data according to the txt, get item as (im1, im2, boolsameperson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "verification_validation_dataset = Verification_dataset(vv_image_one, vv_image_two, vv_issame, val_test_data_transform)\n",
    "verification_validation_dataloader = DataLoader(verification_validation_dataset, batch_size=256,\n",
    "                                                shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "# get some random training images\n",
    "dataiter = iter(verification_validation_dataloader)\n",
    "imone, imtwo, issame = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(imone[0:4]))\n",
    "imshow(torchvision.utils.make_grid(imtwo[0:4]))\n",
    "# print labels\n",
    "print(' '.join('%5s' % issame[j] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Cosine Similarity of Every Pair\n",
    "#### Compare it with the true label in the txt and get the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(dataloader):\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    cossimi = np.zeros(256)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            im1, im2, issame = data[0].to(device), data[1].to(device), data[2]\n",
    "            output1 = net(im1)\n",
    "            output2 = net(im2)\n",
    "            cossimilarity = cos(output1, output2).cpu().numpy()\n",
    "            if i == 0:\n",
    "                cossimi = cossimilarity\n",
    "            else:\n",
    "                cossimi = np.concatenate((cossimi, cossimilarity), axis=0)\n",
    "            if i % 10 == 0:\n",
    "                print(i)\n",
    "    return cossimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = torch.randn(100, 128)\n",
    "input2 = torch.randn(100, 128)\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "output = cos(input1, input2)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate AUC and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "def get_auc(y_true, y_score):\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    return auc\n",
    "\n",
    "y_score = cosine_similarity(verification_validation_dataloader)\n",
    "y_true = vv_issame\n",
    "auc = get_auc(y_true, y_score)\n",
    "print (\"AUC: \", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "threshold = 0.5\n",
    "pred_vv_issame = y_score>threshold\n",
    "print(np.sum(pred_vv_issame))\n",
    "print(np.sum(vv_issame))\n",
    "\n",
    "vv_correct = np.sum(pred_vv_issame == vv_issame)\n",
    "print(\"Accuracy of verification on validation dataset is {}%\".format(vv_correct/len(vv_issame)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and IO Verification\n",
    "### If in the cosine similarity list, the number is larger than threshold, true, otherwise false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "verification_test_dataset = Verification_dataset(tv_image_one, tv_image_two, tv_issame, val_test_data_transform)\n",
    "verification_test_dataloader = DataLoader(verification_test_dataset, batch_size=256,\n",
    "                                                shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "# get some random training images\n",
    "dataiter = iter(verification_test_dataloader)\n",
    "imone, imtwo, issame = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(imone[0:4]))\n",
    "imshow(torchvision.utils.make_grid(imtwo[0:4]))\n",
    "# print labels\n",
    "print(' '.join('%5s' % issame[j] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)\n",
    "y_score_test = cosine_similarity(verification_test_dataloader)\n",
    "print(y_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5 # decide a threshold for verification\n",
    "pred_tf_issame = y_score_test>threshold\n",
    "\n",
    "print(np.sum(pred_tf_issame))\n",
    "print(pred_tf_issame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pandas.core.frame import DataFrame\n",
    "\n",
    "threshold = 0.5 # decide a threshold for verification\n",
    "pred_tf_issame = y_score_test>threshold\n",
    "pred_tf_issame = pred_tf_issame.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "verification_df = pd.read_csv(\"./data/test_trials_verification_student.txt\",header=None,names=['trial'])\n",
    "\n",
    "pred_df = DataFrame(pred_tf_issame, columns=['score'])\n",
    "\n",
    "output_df = pd.concat([verification_df, pred_df], axis=1)\n",
    "\n",
    "\n",
    "output_df.to_csv('out_verification.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tf_issame.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
