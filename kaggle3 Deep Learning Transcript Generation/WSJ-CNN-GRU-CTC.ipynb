{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "taSSqLtNRLaL"
   },
   "source": [
    "# 11785-DeepLearning Sequence to Sequence Translation  `Mar 29`\n",
    "\n",
    "In this homework you will again be working with speech data. We are going to be using\n",
    "unaligned labels in this contest, which means the correlation between the features and labels\n",
    "is not given explicitly and your model will have to igure this out by itself. Hence your data\n",
    "will have a list of phonemes for each utterance, but not which frames correspond to which\n",
    "phonemes.\n",
    "Your main task for this assignment will be to predict the phonemes contained in utterances\n",
    "in the test set. You are not given aligned phonemes in the training data, and you are not\n",
    "asked to produce alignment for the test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HdOqR2rl9W2e"
   },
   "source": [
    "## DataLoader\n",
    "`wsj0 dataloader`: Dataloder will return a matrix of `(time step, 40)` of length `frames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i4n6x1c6RK5t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "class wsj0dataset(Dataset):\n",
    "    def __init__(self, file_dir, label_dir=None):\n",
    "        self.X = np.load(file_dir, allow_pickle=True)\n",
    "        \n",
    "        self.label_dir = label_dir\n",
    "        print(\"Successfully load the data file {0}\".format(file_dir))\n",
    "        self.frame_num = self.X.shape[0]\n",
    "        \n",
    "                  \n",
    "        if label_dir:\n",
    "            self.Y = np.load(label_dir, allow_pickle=True)\n",
    "            print(\"The label set is loaded, shape = ,\", self.Y.shape)\n",
    "        else:\n",
    "            print(\"The data shape is {0}\".format(self.X[0].shape))\n",
    "            \n",
    "        for i in range(1):\n",
    "            print(\"The first data shape is \", self.X[0].shape)\n",
    "            if label_dir:\n",
    "                print(\"The first label shape is\", self.Y[0].shape)\n",
    "                \n",
    "    def __len__(self):  \n",
    "        return self.frame_num\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.label_dir:\n",
    "            \n",
    "            return (torch.from_numpy(self.X[idx]).long(), torch.from_numpy(self.Y[idx]).long())\n",
    "        else:\n",
    "            return torch.from_numpy(self.X[idx]).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPJuz7AMC7uT"
   },
   "source": [
    "### Initialize Train and Dev and Test  dataloader\n",
    "\n",
    "* `wsj0 train.npy`: This file contains your feature data for training the model. It will be\n",
    "of the shape `(frames, time step, 40)`, where the second dimension will be variable as in\n",
    "HW1P2.\n",
    "\n",
    "* `wsj0 dev.npy`: This file is similar to wsj0 train.npy, but should be used to calculate\n",
    "your validation losses and accuracy.\n",
    "\n",
    "* `wsj0 dev.npy`: This file is similar to wsj0 train.npy, but should be used to calculate\n",
    "your validation losses and accuracy.\n",
    "\n",
    "* `wsj0 test.npy`: This file is similar to wsj0 train.npy, but should be used to predict the\n",
    "phoneme labels for the final Kaggle submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_dev_collate(batch):\n",
    "    X = [torch.FloatTensor(item[0].float()) for item in batch]\n",
    "    X = pad_sequence(X)\n",
    "    Y = [torch.FloatTensor(item[1].float()) for item in batch]\n",
    "    Y = pad_sequence(Y, batch_first=True)\n",
    "    \n",
    "    X_lens = torch.FloatTensor([x.size(0) for x, _ in batch])\n",
    "    Y_lens = torch.FloatTensor([y.size(0) for _, y in batch])\n",
    "    return (X, Y, X_lens, Y_lens)\n",
    "\n",
    "def test_collate(batch):\n",
    "    X = [torch.FloatTensor(item.float()) for item in batch]\n",
    "    X = pad_sequence(X)\n",
    "    \n",
    "    X_lens = torch.FloatTensor([x.size(0) for x in batch])\n",
    "    return (X, X_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1147,
     "status": "ok",
     "timestamp": 1571616475677,
     "user": {
      "displayName": "Parth Shah",
      "photoUrl": "",
      "userId": "06799060630439446761"
     },
     "user_tz": 240
    },
    "id": "DvetNUFwrQqd",
    "outputId": "ecedeb67-5e34-4f91-c524-3a116f83f31c"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "def InitDataLoader(root_dir):\n",
    "    kwargs={'num_workers':4, 'pin_memory': True}\n",
    "    # Initialize tarining data\n",
    "    train_path = os.path.join(root_dir, \"wsj0_train\")\n",
    "    train_label_path = os.path.join(root_dir, \"wsj0_train_merged_labels.npy\")\n",
    "    dev_path = os.path.join(root_dir, \"wsj0_dev.npy\")\n",
    "    dev_label_path = os.path.join(root_dir, \"wsj0_dev_merged_labels.npy\")\n",
    "    test_path = os.path.join(root_dir, \"wsj0_test\")\n",
    "    \n",
    "    train_dataloader = DataLoader(wsj0dataset(train_path, train_label_path), \n",
    "                                  batch_size=16,\n",
    "                                  shuffle=True, \n",
    "                                  collate_fn=train_dev_collate,\n",
    "                                  **kwargs)\n",
    "    dev_dataloader = DataLoader(wsj0dataset(dev_path, dev_label_path), \n",
    "                                batch_size=16,\n",
    "                                shuffle=True,\n",
    "                                collate_fn=train_dev_collate,\n",
    "                                **kwargs)\n",
    "    test_dataloader = DataLoader(wsj0dataset(test_path), \n",
    "                                 batch_size=16,\n",
    "                                 shuffle=False,\n",
    "                                 collate_fn=test_collate,\n",
    "                                 **kwargs)\n",
    "    \n",
    "    return train_dataloader, dev_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully load the data file hw3p2/wsj0_train\n",
      "The label set is loaded, shape = , (24724,)\n",
      "The first data shape is  (548, 40)\n",
      "The first label shape is (57,)\n",
      "Successfully load the data file hw3p2/wsj0_dev.npy\n",
      "The label set is loaded, shape = , (1106,)\n",
      "The first data shape is  (958, 40)\n",
      "The first label shape is (100,)\n",
      "Successfully load the data file hw3p2/wsj0_test\n",
      "The data shape is (535, 40)\n",
      "The first data shape is  (535, 40)\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"hw3p2\"\n",
    "train_dataloader, dev_dataloader, test_dataloader = InitDataLoader(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Unit Test for dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i, data in enumerate(test_dataloader):\n",
    "    if i < 1:\n",
    "        print(\"Test Dataloader\")\n",
    "        x, x_len = data\n",
    "        print(x_len)\n",
    "        print(\"the {0}th test_data is {1} and {2}\".format(i, x.size(), x_len.size()))\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "for i, data in enumerate(train_dataloader):\n",
    "    if i < 1:\n",
    "        print(\"Train Dataloader\")\n",
    "        x, y, x_len, y_len = data\n",
    "        print(x_len)\n",
    "        print(\"the {0}th train_data is {1} and {2}\".format(i, x.size(), x_len.size()))\n",
    "    else:\n",
    "        break\n",
    "\n",
    "for i, data in enumerate(dev_dataloader):\n",
    "    if i < 1:\n",
    "        print(\"Train Dataloader\")\n",
    "        x, y, x_len, y_len = data\n",
    "        print(\"the {0}th dev_data is {1} and {2}\".format(i, x.size(), x_len.size()))\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "mNBBNx-xEMc7"
   },
   "source": [
    "## LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 910
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1541,
     "status": "ok",
     "timestamp": 1571616478385,
     "user": {
      "displayName": "Parth Shah",
      "photoUrl": "",
      "userId": "06799060630439446761"
     },
     "user_tz": 240
    },
    "hidden": true,
    "id": "g5tMaKTx9EiS",
    "outputId": "fc704f51-d84b-4ac7-a76a-f1ecbac81594",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class LSTMmodel(nn.Module):\n",
    "    def __init__(self, input_size, out_size, hidden_size):\n",
    "        super(LSTMmodel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.LSTM(input_size,  hidden_size, num_layers = 3, bidirectional = True)\n",
    "        self.output = nn.Linear(hidden_size * 2, out_size)\n",
    "        \n",
    "    def forward(self, X, lengths):\n",
    "        print(\"1. Initail X size,\", X.size())\n",
    "        packed_X = pack_padded_sequence(X, lengths, enforce_sorted=False)\n",
    "        \n",
    "        print(\"2. packed_X size\", packed_X.data.size())\n",
    "        packed_out = self.gru(packed_X)[0]\n",
    "        \n",
    "        print(\"3. packed_out size\", packed_out.data.size())\n",
    "        out, out_lens = pad_packed_sequence(packed_out)\n",
    "        \n",
    "        out = self.output(out).log_softmax(2)\n",
    "        return out, out_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input_size = 40\n",
    "out_size = 47\n",
    "hidden_size = 512\n",
    "\n",
    "test_LSTMmodel = LSTMmodel(input_size, out_size, hidden_size)\n",
    "\n",
    "test_LSTMmodel = test_LSTMmodel.to(device)\n",
    "\n",
    "for i, data in enumerate(dev_dataloader):\n",
    "    if i < 1:\n",
    "        X, Y, X_lens, Y_lens = data\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
    "        X_lens = X_lens.to(device)\n",
    "        Y_lens = Y_lens.to(device)\n",
    "        print(\"X.size() = \", X.size())\n",
    "        print(len(X_lens))\n",
    "        packed_out, final_state = test_LSTMmodel(X, X_lens)\n",
    "        print(packed_out.size())\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LSTM+CNN+FC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class LSTM_CNN_FC_model(nn.Module):\n",
    "    def __init__(self, input_size, channel1, channel2, channel3, lstm_hidden_size, fc1_num, fc2_num, out_size):\n",
    "        super(LSTM_CNN_FC_model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.cnn1 = nn.Conv1d(input_size, channel1, kernel_size=3, stride=1, padding=1) # [64, 1_channel, 1349] (N, C_in, L)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(channel1)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.cnn2 = nn.Conv1d(channel1, channel2, kernel_size=3, stride=1, padding=1) # [64, 2_channel, 1349]\n",
    "        self.batchnorm2 = nn.BatchNorm1d(channel2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.cnn3 = nn.Conv1d(channel2, channel3, kernel_size=3, stride=1, padding=1) # [64, 2_channel, 1349]\n",
    "        self.batchnorm3 = nn.BatchNorm1d(channel3)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2_drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.lstm = nn.GRU(channel3, lstm_hidden_size, num_layers = 3, bidirectional = True, dropout=0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(lstm_hidden_size * 2, fc1_num) # (N, *, H_in) > (N, *, H_out)\n",
    "        self.fc1relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.fc2 = nn.Linear(fc1_num, fc2_num)\n",
    "        self.fc2relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.output = nn.Linear(fc2_num, out_size)\n",
    "        \n",
    "    def forward(self, X, lengths):\n",
    "#         print(\"1. Initial X size,\", X.size())\n",
    "        X = X.transpose(0,1) # [64, 1349, 40] (N, C_in, L)\n",
    "        X = X.transpose(1,2) # [64, 40, 1349] (N, C_in, L)\n",
    "#         print(\"2. Transposed X size,\", X.size())\n",
    "        X = self.cnn1(X)\n",
    "#         print(\"3. CNN1 X size,\", X.size())\n",
    "        X = self.batchnorm1(X)\n",
    "        X = self.relu1(X) # [64, 40, 1349] (N, 1_channel, L)\n",
    "        \n",
    "        X = self.cnn2(X)\n",
    "#         print(\"4. CNN2 X size,\", X.size())\n",
    "        X = self.batchnorm2(X)\n",
    "        X = self.relu2(X) # [64, 40, 1349] (N, 2_channel, L)\n",
    "        \n",
    "        X = self.cnn3(X)\n",
    "#         print(\"4. CNN2 X size,\", X.size())\n",
    "        X = self.batchnorm3(X)\n",
    "        X = self.relu3(X) # [64, 40, 1349] (N, 2_channel, L)\n",
    "        \n",
    "        X = self.conv2_drop(X)\n",
    "        \n",
    "        X = X.transpose(1,2) # [64, 1349, 40] (seq_len, batch, input_size)\n",
    "#         print(\"4. Transpoed X size,\", X.size())\n",
    "        \n",
    "        packed_X = pack_padded_sequence(X, lengths, batch_first = True, enforce_sorted=False)\n",
    "#         print(\"5. packed_X size,\", packed_X.data.size())\n",
    "        \n",
    "        packed_out = self.lstm(packed_X)[0]  # [1349, 64, 47] (seq_len, batch, class_num)\n",
    "#         print(\"6. packed_out size,\", packed_out.data.size())\n",
    "       \n",
    "        out, out_lens = pad_packed_sequence(packed_out, batch_first = True)\n",
    "        \n",
    "        out = self.fc1relu(self.fc1(out)) # [64, 1349, fc1_num]\n",
    "        \n",
    "        out = self.fc2relu(self.fc2(out)) # [64, 1349, fc2_num] \n",
    "        \n",
    "        out = self.output(out).log_softmax(2)\n",
    "        return out, out_lens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM+FC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class LSTM_FC_model(nn.Module):\n",
    "    def __init__(self, input_size, lstm_hidden_size, fc1_num, fc2_num, out_size):\n",
    "        super(LSTM_FC_model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.lstm = nn.GRU(input_size, lstm_hidden_size, num_layers = 3, bidirectional = True, dropout=0.1)\n",
    "        \n",
    "        self.batchnorm1 = nn.BatchNorm1d(lstm_hidden_size * 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(lstm_hidden_size * 2, fc1_num) # (N, *, H_in) > (N, *, H_out)\n",
    "        self.fc1relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.fc2 = nn.Linear(fc1_num, fc2_num)\n",
    "        self.fc2relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.output = nn.Linear(fc2_num, out_size)\n",
    "        \n",
    "    def forward(self, X, lengths):\n",
    "#         print(\"1. Initial X size,\", X.size())\n",
    "        X = X.transpose(0,1) # [64, 1349, 40] (N, C_in, L)\n",
    "#         X = X.transpose(1,2) # [64, 40, 1349] (N, C_in, L)\n",
    "#         print(\"2. Transposed X size,\", X.size())\n",
    "        \n",
    "#         X = X.transpose(0,1) # [64, 1349, 40] (N, C_in, L)\n",
    "        packed_X = pack_padded_sequence(X, lengths, batch_first = True, enforce_sorted=False)\n",
    "#         print(\"5. packed_X size,\", packed_X.data.size())\n",
    "        \n",
    "        packed_out = self.lstm(packed_X)[0]  # [1349, 64, 47] (seq_len, batch, class_num)\n",
    "#         print(\"6. packed_out size,\", packed_out.data.size())\n",
    "       \n",
    "        out, out_lens = pad_packed_sequence(packed_out, batch_first = True)\n",
    "#         print(\"out\", out.size())\n",
    "        \n",
    "        out = out.transpose(1,2)        \n",
    "        out = self.batchnorm1(out)\n",
    "#         print(\"out\", out.size())\n",
    "        out = out.transpose(1,2)\n",
    "#         print(\"out\", out.size())\n",
    "        out = self.fc1relu(self.fc1(out)) # [64, 1349, fc1_num]\n",
    "        \n",
    "        out = self.fc2relu(self.fc2(out)) # [64, 1349, fc2_num] \n",
    "        \n",
    "        out = self.output(out).log_softmax(2)\n",
    "        return out, out_lens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Unit Test For model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.size() =  torch.Size([1368, 16, 40])\n",
      "2. Transposed X size, torch.Size([16, 1368, 40])\n",
      "out torch.Size([16, 1368, 2048])\n",
      "out torch.Size([16, 2048, 1368])\n",
      "out torch.Size([16, 1368, 2048])\n",
      "Log_probs size =  torch.Size([1368, 16, 47])\n",
      "Targets size =  torch.Size([16, 156])\n",
      "Input_lengths =  torch.Size([16])\n",
      "Target_lengths =  torch.Size([16])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "input_size = 40\n",
    "out_size = 47\n",
    "hidden_size = 512\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "test_LSTMmodel = LSTM_FC_model(input_size = 40,\n",
    "                               lstm_hidden_size = 1024, \n",
    "                               fc1_num = 2048, \n",
    "                               fc2_num = 1024, \n",
    "                               out_size = 47)\n",
    "\n",
    "test_LSTMmodel = test_LSTMmodel.to(device)\n",
    "criterion = nn.CTCLoss()\n",
    "for i, data in enumerate(dev_dataloader):\n",
    "    torch.cuda.empty_cache()\n",
    "    if i < 1:\n",
    "        X, Y, X_lens, Y_lens = data\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device).int()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
    "        X_lens = X_lens.to(device).int()\n",
    "        Y_lens = Y_lens.to(device).int()\n",
    "        print(\"X.size() = \", X.size())\n",
    "        out, out_lens = test_LSTMmodel(X, X_lens)\n",
    "        \n",
    "        out = out.transpose(0,1)\n",
    "        print(\"Log_probs size = \", out.size())\n",
    "        print(\"Targets size = \", Y.size())\n",
    "        print(\"Input_lengths = \", out_lens.size())\n",
    "        print(\"Target_lengths = \", Y_lens.size())\n",
    "                                       \n",
    "        loss = criterion(out, Y, out_lens, Y_lens)\n",
    "        print(out_lens.size())\n",
    "        del X,Y,X_lens,Y_lens, out, out_lens, loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "del test_LSTMmodel\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def debug_memory():\n",
    "    import collections, gc, resource, torch\n",
    "    print('maxrss = {}'.format(\n",
    "        resource.getrusage(resource.RUSAGE_SELF).ru_maxrss))\n",
    "    tensors = collections.Counter((str(o.device), o.dtype, tuple(o.shape))\n",
    "                                  for o in gc.get_objects()\n",
    "                                  if torch.is_tensor(o))\n",
    "    for line in tensors.items():\n",
    "        print('{}\\t{}'.format(line[0], line[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxrss = 6216816\n",
      "('cuda:0', torch.float32, (512, 40, 3))\t4\n",
      "('cuda:0', torch.float32, (512,))\t32\n",
      "('cuda:0', torch.float32, (512, 512, 3))\t4\n",
      "('cuda:0', torch.float32, (6144, 512))\t8\n",
      "('cuda:0', torch.float32, (6144, 2048))\t24\n",
      "('cuda:0', torch.float32, (6144,))\t48\n",
      "('cuda:0', torch.float32, (6144, 4096))\t16\n",
      "('cuda:0', torch.float32, (1024, 4096))\t4\n",
      "('cuda:0', torch.float32, (1024,))\t8\n",
      "('cuda:0', torch.float32, (1024, 1024))\t4\n",
      "('cuda:0', torch.float32, (47, 1024))\t4\n",
      "('cuda:0', torch.float32, (47,))\t4\n",
      "('cpu', torch.float32, (1256, 32, 40))\t1\n",
      "('cpu', torch.float32, (32, 143))\t1\n",
      "('cpu', torch.float32, (32,))\t4\n",
      "('cuda:0', torch.float32, (1256, 32, 40))\t1\n",
      "('cuda:0', torch.int32, (32, 143))\t1\n",
      "('cpu', torch.int32, (32,))\t4\n",
      "('cuda:0', torch.int64, ())\t4\n",
      "('cuda:0', torch.float32, (32, 1256, 512))\t1\n",
      "('cuda:0', torch.int64, (32,))\t2\n",
      "('cuda:0', torch.float32, (22636, 512))\t1\n",
      "('cpu', torch.int64, (1256,))\t1\n",
      "('cuda:0', torch.float32, (6, 32, 2048))\t1\n",
      "('cuda:0', torch.float32, (1145, 32, 40))\t1\n",
      "('cuda:0', torch.int32, (32, 142))\t1\n",
      "('cuda:0', torch.float32, (1145, 32, 47))\t1\n",
      "('cpu', torch.int64, (32,))\t1\n",
      "('cuda:0', torch.float32, ())\t3\n",
      "('cpu', torch.float32, (32, 142))\t1\n",
      "('cpu', torch.float32, (1145, 32, 40))\t1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lldlld0574_gmail_com/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "debug_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html#ctcloss\n",
    "\n",
    "`nn.CTCLoss` takes 4 arguments to compute the loss:\n",
    "* `log_probs`: Prediction of your model at each time step.\n",
    "  * Shape: (T, N, C), where T is the largest length in the batch, N is batch size, and C is number of classes (remember that it should be number of phonemes plus 1).\n",
    "  * **Values must be log probabilities.** Neither probabilities nor logits will work. Make sure the output of your network is log probabilities, by adding a `nn.LogSoftmax` after the last linear layer.\n",
    "* `targets`: The ground truth sequences.\n",
    "  * Shape: (N, S), where N is batch size, and S is the largest length in the batch. **WARNING!** This dimension order is unconventional in PyTorch. If you use `torch.nn.utils.rnn.pad_sequence` to pad the target sequence,  **you must explicitly set `batch_first=True`**.\n",
    "  * Values are indices of phonemes. Again, remember that index 0 is reserved for \"blank\" and should not represent any phoneme.\n",
    "* `input_lengths`: Lengths of sequences in `log_probs`.\n",
    "  * Shape: (N,).\n",
    "  * This is not necessarily the same as lengths of input of the model. If your model uses CNNs or pyramidal RNNs, it changes the length of sequences, and you must correctly compute the lengths of its output to be used here.\n",
    "* `target_lengths`: Lengths of sequences in `targets`.\n",
    "  * Shape: (N,).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(model, input_size, out_size, criterion, decoder, PHONEME_MAP, file_name, learning_rate):\n",
    "    torch.manual_seed(11785)\n",
    "    loss_history = []\n",
    "    log_interval = 20\n",
    "    for epoch in range(800):\n",
    "        loss_interval = 0\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01*learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.9)\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "#             print(i)\n",
    "            X, Y, X_lens, Y_lens = data\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device).int()\n",
    "#             print(\"Y_lens.size()\", Y_lens.size())\n",
    "            Y_lens = Y_lens.int()\n",
    "            X_lens = X_lens.int()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out, out_lens = model(X, X_lens)\n",
    "#             print(\"out_lens.size()\", out_lens.size())\n",
    "            \n",
    "            out = out.transpose(0,1)\n",
    "            \n",
    "            \n",
    "            loss = criterion(out, Y, out_lens, Y_lens)\n",
    "            loss.backward()\n",
    "#             print(loss.item())\n",
    "            loss_interval += loss.item()\n",
    "            loss_history.append(loss.item)\n",
    "            optimizer.step()\n",
    "            loss_interval=loss.item()\n",
    "#             try:\n",
    "#                 loss = criterion(out, Y, out_lens, Y_lens)\n",
    "#                 loss.backward()\n",
    "#                 loss_interval += loss.item()\n",
    "#                 loss_history.append(loss.item)\n",
    "#                 optimizer.step()\n",
    "#             except:\n",
    "#                 print(\"Last batch, skip\")\n",
    "            \n",
    "            if i%log_interval == 0:\n",
    "                print('Epoch', epoch + 1, 'Sample', i, 'Loss',loss_interval/log_interval)\n",
    "                loss_interval = 0\n",
    "                torch.save(model.state_dict(), file_name)\n",
    "            del X, Y, X_lens, Y_lens, out, out_lens\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        \n",
    "        \n",
    "        evaluate(model, 16, decoder, PHONEME_MAP)\n",
    "#         scheduler.step(loss)\n",
    "        scheduler.step()\n",
    "        torch.save(model.state_dict(), file_name)\n",
    "        print('Epoch', epoch + 1, 'Loss', loss.item())\n",
    "        learning_rate = 0.1 * learning_rate\n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATES = 138\n",
    "N_PHONEMES = N_STATES // 3\n",
    "PHONEME_LIST = [\n",
    "    \"\",\n",
    "    \"+BREATH+\",\n",
    "    \"+COUGH+\",\n",
    "    \"+NOISE+\",\n",
    "    \"+SMACK+\",\n",
    "    \"+UH+\",\n",
    "    \"+UM+\",\n",
    "    \"AA\",\n",
    "    \"AE\",\n",
    "    \"AH\",\n",
    "    \"AO\",\n",
    "    \"AW\",\n",
    "    \"AY\",\n",
    "    \"B\",\n",
    "    \"CH\",\n",
    "    \"D\",\n",
    "    \"DH\",\n",
    "    \"EH\",\n",
    "    \"ER\",\n",
    "    \"EY\",\n",
    "    \"F\",\n",
    "    \"G\",\n",
    "    \"HH\",\n",
    "    \"IH\",\n",
    "    \"IY\",\n",
    "    \"JH\",\n",
    "    \"K\",\n",
    "    \"L\",\n",
    "    \"M\",\n",
    "    \"N\",\n",
    "    \"NG\",\n",
    "    \"OW\",\n",
    "    \"OY\",\n",
    "    \"P\",\n",
    "    \"R\",\n",
    "    \"S\",\n",
    "    \"SH\",\n",
    "    \"SIL\",\n",
    "    \"T\",\n",
    "    \"TH\",\n",
    "    \"UH\",\n",
    "    \"UW\",\n",
    "    \"V\",\n",
    "    \"W\",\n",
    "    \"Y\",\n",
    "    \"Z\",\n",
    "    \"ZH\"\n",
    "]\n",
    "\n",
    "PHONEME_MAP = [\n",
    "    '',\n",
    "    '_',  # \"+BREATH+\"\n",
    "    '+',  # \"+COUGH+\"\n",
    "    '~',  # \"+NOISE+\"\n",
    "    '!',  # \"+SMACK+\"\n",
    "    '-',  # \"+UH+\"\n",
    "    '@',  # \"+UM+\"\n",
    "    'a',  # \"AA\"\n",
    "    'A',  # \"AE\"\n",
    "    'h',  # \"AH\"\n",
    "    'o',  # \"AO\"\n",
    "    'w',  # \"AW\"\n",
    "    'y',  # \"AY\"\n",
    "    'b',  # \"B\"\n",
    "    'c',  # \"CH\"\n",
    "    'd',  # \"D\"\n",
    "    'D',  # \"DH\"\n",
    "    'e',  # \"EH\"\n",
    "    'r',  # \"ER\"\n",
    "    'E',  # \"EY\"\n",
    "    'f',  # \"F\"\n",
    "    'g',  # \"G\"\n",
    "    'H',  # \"HH\"\n",
    "    'i',  # \"IH\"\n",
    "    'I',  # \"IY\"\n",
    "    'j',  # \"JH\"\n",
    "    'k',  # \"K\"\n",
    "    'l',  # \"L\"\n",
    "    'm',  # \"M\"\n",
    "    'n',  # \"N\"\n",
    "    'G',  # \"NG\"\n",
    "    'O',  # \"OW\"\n",
    "    'Y',  # \"OY\"\n",
    "    'p',  # \"P\"\n",
    "    'R',  # \"R\"\n",
    "    's',  # \"S\"\n",
    "    'S',  # \"SH\"\n",
    "    '.',  # \"SIL\"\n",
    "    't',  # \"T\"\n",
    "    'T',  # \"TH\"\n",
    "    'u',  # \"UH\"\n",
    "    'U',  # \"UW\"\n",
    "    'v',  # \"V\"\n",
    "    'W',  # \"W\"\n",
    "    '?',  # \"Y\"\n",
    "    'z',  # \"Z\"\n",
    "    'Z',  # \"ZH\"\n",
    "]\n",
    "\n",
    "assert len(PHONEME_LIST) == len(PHONEME_MAP)\n",
    "assert len(set(PHONEME_MAP)) == len(PHONEME_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-Levenshtein in /home/lldlld0574_gmail_com/anaconda3/lib/python3.7/site-packages (0.12.0)\r\n",
      "Requirement already satisfied: setuptools in /home/lldlld0574_gmail_com/anaconda3/lib/python3.7/site-packages (from python-Levenshtein) (41.4.0)\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Levenshtein import distance\n",
    "distance('kitten', 'sitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, batch_size, decoder, PHONEME_MAP):\n",
    "    with torch.no_grad():\n",
    "        batch_amount = 0\n",
    "        loss = 0\n",
    "        distances = 0\n",
    "        overall_data_num = 0\n",
    "        for i, data in enumerate(dev_dataloader):\n",
    "            \n",
    "            X, Y, X_lens, Y_lens = data\n",
    "#             print(Y_lens.size())\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device).int()\n",
    "            Y_lens = Y_lens.int()\n",
    "            X_lens = X_lens.int()\n",
    "#             print(X.type(),X_lens.type())\n",
    "            \n",
    "            out, out_lens = model(X, X_lens)\n",
    "#             print(\"out.size()=\",out.size())\n",
    "#             print(\"out_lens.size()=\",out_lens.size())\n",
    "            out = out.transpose(0,1)\n",
    "            \n",
    "            loss += criterion(out, Y, out_lens, Y_lens)\n",
    "            eval_Y, _, _, eval_Y_lens = decoder.decode(out.transpose(0, 1), out_lens)\n",
    "#             print(\"eval_Y.size()=\",eval_Y.size())\n",
    "#             print(\"eval_Y_lens.size()=\",eval_Y_lens.size())\n",
    "#             print(\"Y.size()=\",Y.size())\n",
    "#             print(\"lens.size()=\",Y_lens.size())\n",
    "            overall_data_num += eval_Y.size(0)\n",
    "            for j in range(eval_Y.size(0)):\n",
    "                eval_best_idx = eval_Y[j, 0, 0:eval_Y_lens[j,0]]\n",
    "                eval_best_phoneme = \"\"\n",
    "                for phoneme_idx in eval_best_idx:\n",
    "                    eval_best_phoneme+=PHONEME_LIST[phoneme_idx]\n",
    "\n",
    "                best_idx = Y[j,0:Y_lens[j]]\n",
    "                best_phoneme = \"\"\n",
    "                for phoneme_idx in best_idx:\n",
    "                    best_phoneme+=PHONEME_LIST[phoneme_idx]\n",
    "#                 print(\"eval_best_phoneme = \", eval_best_phoneme,\"best_phoneme = \", best_phoneme)\n",
    "                distances += distance(eval_best_phoneme,best_phoneme)\n",
    "            \n",
    "            del X, Y, X_lens, Y_lens, out, out_lens\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        distances /= (overall_data_num * 2)\n",
    "        print(\"distance = \", distances)\n",
    "            \n",
    "            \n",
    "        av_loss = loss/batch_amount\n",
    "        print(\"evaluation_loss = \", av_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ctcdecode import CTCBeamDecoder\n",
    "input_size = 40\n",
    "out_size = 47\n",
    "hidden_size = 512\n",
    "batch_size = 16\n",
    "# file_name = 'cnn2_lstm3_fc2_large'\n",
    "# file_name = 'cnn3_lstm3_dropout_fc2_large'\n",
    "file_name = 'lstm3_dropout_fc2_large'\n",
    "\n",
    "try: \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    print(\"No model in storage\")\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# model = LSTMmodel(input_size, out_size, hidden_size).to(device)\n",
    "# model = LSTM_CNN_FC_model(input_size = 40,\n",
    "#                            channel1 = 512, \n",
    "#                            channel2 = 512, \n",
    "#                            lstm_hidden_size = 2048, \n",
    "#                            fc1_num = 1024, \n",
    "#                            fc2_num = 1024, \n",
    "#                            out_size = 47).to(device)\n",
    "# model = LSTM_CNN_FC_model(input_size = 40,\n",
    "#                            channel1 = 128, \n",
    "#                            channel2 = 256, \n",
    "#                            channel3 = 512,\n",
    "#                            lstm_hidden_size = 2048, \n",
    "#                            fc1_num = 1024, \n",
    "#                            fc2_num = 1024, \n",
    "#                            out_size = 47).to(device)\n",
    "model = LSTM_FC_model(input_size = 40,\n",
    "                      lstm_hidden_size = 2048, \n",
    "                      fc1_num = 1024, \n",
    "                      fc2_num = 1024, \n",
    "                      out_size = 47).to(device)\n",
    "model.load_state_dict(torch.load(file_name, map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Sample 0 Loss 0.034434324502944945\n",
      "Epoch 1 Sample 20 Loss 0.03113899230957031\n",
      "Epoch 1 Sample 40 Loss 0.039797359704971315\n",
      "Epoch 1 Sample 60 Loss 0.030459046363830566\n",
      "Epoch 1 Sample 80 Loss 0.03343487977981567\n",
      "Epoch 1 Sample 100 Loss 0.03189215958118439\n",
      "Epoch 1 Sample 120 Loss 0.03129576444625855\n",
      "Epoch 1 Sample 140 Loss 0.041374611854553225\n",
      "Epoch 1 Sample 160 Loss 0.036687979102134706\n",
      "Epoch 1 Sample 180 Loss 0.036330944299697875\n",
      "Epoch 1 Sample 200 Loss 0.035518613457679746\n",
      "Epoch 1 Sample 220 Loss 0.036587494611740115\n",
      "Epoch 1 Sample 240 Loss 0.03303710520267487\n",
      "Epoch 1 Sample 260 Loss 0.042043089866638184\n",
      "Epoch 1 Sample 280 Loss 0.03513888418674469\n",
      "Epoch 1 Sample 300 Loss 0.025820434093475342\n",
      "Epoch 1 Sample 320 Loss 0.03313668072223663\n",
      "Epoch 1 Sample 340 Loss 0.029886752367019653\n",
      "Epoch 1 Sample 360 Loss 0.029035082459449767\n",
      "Epoch 1 Sample 380 Loss 0.035458290576934816\n",
      "Epoch 1 Sample 400 Loss 0.03287593424320221\n",
      "Epoch 1 Sample 420 Loss 0.0379438728094101\n",
      "Epoch 1 Sample 440 Loss 0.030978658795356752\n",
      "Epoch 1 Sample 460 Loss 0.03459475040435791\n",
      "Epoch 1 Sample 480 Loss 0.03407637476921081\n",
      "Epoch 1 Sample 500 Loss 0.0351237952709198\n",
      "Epoch 1 Sample 520 Loss 0.03252035677433014\n",
      "Epoch 1 Sample 540 Loss 0.03101615607738495\n",
      "Epoch 1 Sample 560 Loss 0.03495112061500549\n",
      "Epoch 1 Sample 580 Loss 0.03696770071983337\n",
      "Epoch 1 Sample 600 Loss 0.027678856253623964\n",
      "Epoch 1 Sample 620 Loss 0.0354116678237915\n",
      "Epoch 1 Sample 640 Loss 0.03248167037963867\n",
      "Epoch 1 Sample 660 Loss 0.032550132274627684\n",
      "Epoch 1 Sample 680 Loss 0.037656909227371214\n",
      "Epoch 1 Sample 700 Loss 0.05928046107292175\n",
      "Epoch 1 Sample 720 Loss 0.03312453031539917\n",
      "Epoch 1 Sample 740 Loss 0.03203674256801605\n",
      "Epoch 1 Sample 760 Loss 0.03214613795280456\n",
      "Epoch 1 Sample 780 Loss 0.035495412349700925\n",
      "Epoch 1 Sample 800 Loss 0.03807286024093628\n",
      "Epoch 1 Sample 820 Loss 0.03423532247543335\n",
      "Epoch 1 Sample 840 Loss 0.036910155415534975\n",
      "Epoch 1 Sample 860 Loss 0.03647783100605011\n",
      "Epoch 1 Sample 880 Loss 0.038477030396461484\n",
      "Epoch 1 Sample 900 Loss 0.037297886610031125\n",
      "Epoch 1 Sample 920 Loss 0.03320805430412292\n",
      "Epoch 1 Sample 940 Loss 0.03799029588699341\n",
      "Epoch 1 Sample 960 Loss 0.037465518712997435\n",
      "Epoch 1 Sample 980 Loss 0.03088286519050598\n",
      "Epoch 1 Sample 1000 Loss 0.040422174334526065\n",
      "Epoch 1 Sample 1020 Loss 0.049855396151542664\n",
      "Epoch 1 Sample 1040 Loss 0.028713107109069824\n",
      "Epoch 1 Sample 1060 Loss 0.03553962707519531\n",
      "Epoch 1 Sample 1080 Loss 0.04076094031333923\n",
      "Epoch 1 Sample 1100 Loss 0.03568602204322815\n",
      "Epoch 1 Sample 1120 Loss 0.046615034341812134\n",
      "Epoch 1 Sample 1140 Loss 0.03616721332073212\n",
      "Epoch 1 Sample 1160 Loss 0.03744053542613983\n",
      "Epoch 1 Sample 1180 Loss 0.03462914228439331\n",
      "Epoch 1 Sample 1200 Loss 0.04269352257251739\n",
      "Epoch 1 Sample 1220 Loss 0.041408923268318173\n",
      "Epoch 1 Sample 1240 Loss 0.04179816246032715\n",
      "Epoch 1 Sample 1260 Loss 0.029243519902229308\n",
      "Epoch 1 Sample 1280 Loss 0.03210785388946533\n",
      "Epoch 1 Sample 1300 Loss 0.03937115073204041\n",
      "Epoch 1 Sample 1320 Loss 0.037618201971054074\n",
      "Epoch 1 Sample 1340 Loss 0.10487370491027832\n",
      "Epoch 1 Sample 1360 Loss 0.03408203125\n",
      "Epoch 1 Sample 1380 Loss 0.06160715818405151\n",
      "Epoch 1 Sample 1400 Loss 0.03955356776714325\n",
      "Epoch 1 Sample 1420 Loss 0.03803257346153259\n",
      "Epoch 1 Sample 1440 Loss 0.03951302170753479\n",
      "Epoch 1 Sample 1460 Loss 0.03962528705596924\n",
      "Epoch 1 Sample 1480 Loss 0.06930301189422608\n",
      "Epoch 1 Sample 1500 Loss 0.03843608200550079\n",
      "Epoch 1 Sample 1520 Loss 0.03733263313770294\n",
      "Epoch 1 Sample 1540 Loss 0.0403894305229187\n",
      "distance =  20.17269439421338\n",
      "evaluation_loss =  tensor(inf, device='cuda:0')\n",
      "Epoch 1 Loss 0.833457350730896\n",
      "Epoch 2 Sample 0 Loss 0.029892218112945557\n",
      "Epoch 2 Sample 20 Loss 0.04196748733520508\n",
      "Epoch 2 Sample 40 Loss 0.033453932404518126\n",
      "Epoch 2 Sample 60 Loss 0.03368196189403534\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CTCLoss()\n",
    "learning_rate = 1e-6\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-7)\n",
    "decoder = CTCBeamDecoder(['$'] * (len(PHONEME_MAP)), beam_width=10, num_processes = os.cpu_count(), log_probs_input=True)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.9)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=0)\n",
    "\n",
    "# evaluate(model, batch_size, decoder, PHONEME_MAP)\n",
    "train(model, input_size, out_size, criterion, decoder, PHONEME_MAP, file_name, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "`CTCBeamDecoder`:\n",
    "* `phonemes`: **It doesn't need to be actual phonemes.** The only requirement is being a list of characters whose length is the number of classes (number of phonemes plus 1). \n",
    "* `beam_width`: Larger beam width produces better output, but also costs more time and memory.\n",
    "* `num_processes`: Number of processes for parallel decoding. Setting it to `os.cpu_count()` is recommended as it utilizes all CPU cores.\n",
    "* `log_probs_input`: Should always be True, since your model output is log probabilities.\n",
    "\n",
    "`CTCBeamDecoder.decode` arguments:\n",
    "* `probs`: Prediction from your model as log probabilities (if `log_probs_input=True`).\n",
    "  * Shape: (N, T, C). where N is batch size, T is the largest length in the batch, and C is number of classes. **WARNING!** This dimension order is unconventional in PyTorch. You likely need to do `out.transpose(0, 1)` on your output.\n",
    "* `len`: Lengths of sequences in `probs`.\n",
    "  * Shape: (N,)\n",
    "\n",
    "\n",
    "`CTCBeamDecoder.decode` return value (tuple of 4):\n",
    "* First item `output`: Decoded top sequences.\n",
    "  * Shape: (N, B, T), where B is the beam width. Normally we only need the best sequences, which are indexed 0 in the second (beam width) dimension.\n",
    "* Second and third can be ignored.\n",
    "* Last item `out_seq_len`: Length of sequences in `output`. \n",
    "  * Shape: (N, B). Lengths of best sequences are indexed 0 in the second (beam width) dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def test(model, decoder, PHONEME_LIST):\n",
    "    result_phoneme = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_dataloader):\n",
    "            \n",
    "            X, X_lens = data\n",
    "            X = X.to(device)\n",
    "            X_lens = X_lens.int()\n",
    "            out, out_lens = model(X, X_lens)\n",
    "            out = out.cpu()\n",
    "            \n",
    "            test_Y, _, _, test_Y_lens = decoder.decode(out, out_lens)\n",
    "            \n",
    "            for j in range(test_Y.size(0)):\n",
    "                best_idx = test_Y[j, 0, 0:test_Y_lens[j,0]]\n",
    "                best_phoneme = \"\"\n",
    "                for phoneme_idx in best_idx:\n",
    "                    if phoneme_idx!=0:\n",
    "                        best_phoneme+=PHONEME_LIST[phoneme_idx+1] #beacuse the first is the blank\n",
    "                result_phoneme.append(best_phoneme)\n",
    "    return result_phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = CTCBeamDecoder(['$'] * (len(PHONEME_MAP)), beam_width=10, num_processes = os.cpu_count(), log_probs_input=True)\n",
    "predict_phoneme = test(model, decoder, PHONEME_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Test Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of id =  523\n",
      "len of phoneme =  523\n",
      "      id                                          Predicted\n",
      "0      0  .inshmhlkalWhzbestwnAdDhnAksywbhnpRizhndhnDhka...\n",
      "1      1  .DhssOvIitskamhInWylkamhHAvbikhnWiTgRyntRUzfam...\n",
      "2      2  .RyhfRIsklzWrfoRmmOTimpoRnErHijAnAnmithklAskhm...\n",
      "3      3  .gREnzhnsYbiGDiskOlhnDhtRedhvREnhnfEvrhbhlzRiG...\n",
      "4      4  .?UnyndistEtskhndrtukptUdIfAndWestrn?UROphgens...\n",
      "..   ...                                                ...\n",
      "518  518  .hSevRanpethththlOfyvHhntrfitITrmil?hndalrSlhs...\n",
      "519  519  .RelIzWrmhtikhlstIstEjdtUIhnfoRsDhkempEnzmeshn...\n",
      "520  520  .milhteIpalhsIWhztikIphtRhvhlRwtsObhnhndpRhtek...\n",
      "521  521  .hnDhpOgrWIkkWORnbOthndsoRghnfoRmrzkhnRhsIvnyn...\n",
      "522  522  .innyntInEtIEtkamhaprEtiGhnkamWizrRekidfoRHhnt...\n",
      "\n",
      "[523 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "id = [x for x in range(0, 523)]\n",
    "print(\"len of id = \", len(id))\n",
    "print(\"len of phoneme = \", len(predict_phoneme))\n",
    "\n",
    "result = {'id':id, 'Predicted':predict_phoneme}\n",
    "df = pd.DataFrame(result, columns= ['id', 'Predicted'])\n",
    "print(df)\n",
    "df.to_csv (file_name+'.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CTC.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
